# ğŸ“š Chat With My PDFs â€” Local, Private, Free

Implements a privacy-focused, fully local RAG pipeline (Retrieval-Augmented Generation) for PDF semantic search & chat â€” all offline.

---

## What this project does

- Lets you upload **one or more PDFs**, parse them locally, and search them semantically.
- Extracts plain text, tables, and performs OCR if needed.
- Embeds each chunk using a lightweight **locally-run** SentenceTransformer.
- Stores embeddings in a **FAISS** vector database for fast similarity search.
- Uses **Ollama** to run a **local LLM** (like Mistral or Llama3) that answers your questions *using only your PDFs*.
- Runs fully offline â€” no cloud APIs â€” your PDFs stay private.
- Current version returns the top 3 closest matches with full chunk context.

---

## Example: What a chunk looks like

```json
{
  "pdf": "pdfs/ATOMS-AND-MOLECULES.pdf",
  "page": 5,
  "chunk_id": "text_0",
  "type": "text",
  "content": "Dalton's Atomic Theory states that all matter is made up of indivisible atoms..."
}
```

Each chunk is embedded into the vector store for semantic search.
When you query, you get back the top chunks with their source info â€” so you can trace results back to the exact page in your PDF.

---

## How Ollama fits in

1. **Set up Ollama**:  
   - Installed with `curl -fsSL https://ollama.com/install.sh | sh`  
   - Pulled a local model: `ollama pull mistral` or `ollama pull llama3`  
   - Tested standalone:  
     ```bash
     ollama run mistral
     ```

2. **Integrated with the app**:  
   - The `ollama` Python client connects to `localhost:11434`.
   - When you ask a question, your local vector DB finds the top chunks.
   - The top chunks are used as **context** in the prompt.
   - Ollama answers *only* from that context â€” no internet calls.

---

## How Streamlit builds the UI

Your local app uses [Streamlit](https://streamlit.io/) to run like a mini web app:
- **PDF uploader**: drag & drop one or more PDFs.
- Shows **spinner/progress** while parsing, chunking & embedding.
- **Ask a question**: simple text box with a â€œthinkingâ€¦â€ spinner.
- **Prompt inspector**: shows what context the LLM sees.
- **Source Chunks**: see which text chunks answered your question.
- **Sidebar chat history**: so you can trace what you asked and what it answered.

---

## Project structure

```
chat-with-my-pdf/
â”œâ”€â”€ app.py                  # Your Streamlit web app
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parser.py           # Extracts text, tables, OCR
â”œâ”€â”€ pdfs/                   # PDFs you upload go here (local only)
â”œâ”€â”€ faiss.index             # Your local vector DB
â”œâ”€â”€ chunks_metadata.json    # Metadata to trace chunks to pages
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## How to run it

### 1ï¸âƒ£ Clone & set up your virtual environment

```bash
git clone https://github.com/Mrunmoy/chat-with-my-pdf.git
cd chat-with-my-pdf

python3 -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Run Ollama in the background

```bash
ollama serve
```
Or use the default `systemd` service if installed automatically.

---

### 3ï¸âƒ£ Start your Streamlit app

```bash
streamlit run app.py
```

It will open [http://localhost:8501](http://localhost:8501) â€” your local chat window.

---

### 4ï¸âƒ£ Upload, chat, repeat!

- Drag in one or more PDFs.
- Watch the spinner as they get parsed & chunked.
- A new vector DB is built every time.
- Ask any question â€” your local LLM answers using your PDFs only.

---

## Example: Local pipeline in action

![Multi PDF Upload Demo](docs/screenshot.png)

---

## Why this is private

- Your PDFs, index, and chunks stay local.
- Your `.gitignore` ensures you never commit private `pdfs/`, `faiss.index`, or `chunks_metadata.json`.
- Ollama runs your LLM offline.

---

## Running it with Docker Compose ğŸ³ + `start.sh`

This project includes a simple helper script **`start.sh`** that wraps up the whole multi-step process:
- âœ… Builds your Docker images
- âœ… Starts the Ollama server in the background
- âœ… Waits until the Ollama API is actually ready (not just â€œstartedâ€ â€” but responding to requests!)
- âœ… Pulls your chosen local model (e.g., `mistral`) exactly once
- âœ… Spins up your Streamlit app

**Why the wait?**  
Ollamaâ€™s `pull` needs the **`ollama serve`** HTTP API to be running.  
Docker Composeâ€™s `depends_on` only checks if the container is *running* â€” not if its server is actually ready to accept connections.  
Thatâ€™s why the `start.sh` includes a small `until ... do ... done` loop to poll until `ollama list` works reliably.

---

## âœ… Example: `start.sh`

```bash
#!/bin/bash

set -e

echo "Building images *if needed* and starting Ollama server..."
docker-compose up --build -d ollama_server

echo "â³ Waiting for Ollama to pass healthcheck..."

while ! curl -s http://localhost:11434 | grep -q 'Ollama'; do
  echo "ğŸ”„ Ollama not ready yet â€” waiting..."
  sleep 3
done

echo "âœ… Ollama is healthy!"

echo "Pulling the model..."
docker-compose exec ollama_server ollama pull mistral

echo "Bringing up Streamlit app..."
docker-compose up -d chatwithpdf

echo "All done!"

```

### ğŸŸ¢ How to use it

Make it executable:
```bash
chmod +x start.sh
```

Run it:
```bash
./start.sh

```

Open http://<your-ip>:8501 in your browser.


## Built with

- [PyMuPDF](https://pymupdf.readthedocs.io/) â€” PDF parsing
- [pdfplumber](https://github.com/jsvine/pdfplumber) â€” Tables
- [pytesseract](https://github.com/madmaze/pytesseract) â€” OCR
- [sentence-transformers](https://www.sbert.net/) â€” Local embeddings
- [FAISS](https://github.com/facebookresearch/faiss) â€” Local vector DB
- [Ollama](https://ollama.com) â€” Run local LLMs
- [Streamlit](https://streamlit.io/) â€” Web UI

---


## âš–ï¸ License
MIT â€” for personal learning and research.  
Respect any copyright for your own PDFs.

---

Thatâ€™s it! ğŸ“šâœ¨  
**Upload â†’ Process â†’ Chat â†’ Learn â€” all local, all yours.**

