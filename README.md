# ğŸ“š Chat With My PDFs â€” Local, Private, Free

Implements a privacy-focused, fully local RAG pipeline (Retrieval-Augmented Generation) for PDF semantic search & chat â€” all offline.

---

## What this project does

- Lets you upload **one or more PDFs**, parse them locally, and search them semantically.
- Extracts plain text, tables, and performs OCR if needed.
- Embeds each chunk using a lightweight **locally-run** SentenceTransformer.
- Stores embeddings in a **FAISS** vector database for fast similarity search.
- Uses **Ollama** to run a **local LLM** (like Mistral or Llama3) that answers your questions *using only your PDFs*.
- Runs fully offline â€” no cloud APIs â€” your PDFs stay private.
- Current version returns the top 3 closest matches with full chunk context.

---

## Example: What a chunk looks like

```json
{
  "pdf": "pdfs/ATOMS-AND-MOLECULES.pdf",
  "page": 5,
  "chunk_id": "text_0",
  "type": "text",
  "content": "Dalton's Atomic Theory states that all matter is made up of indivisible atoms..."
}
```

Each chunk is embedded into the vector store for semantic search.
When you query, you get back the top chunks with their source info â€” so you can trace results back to the exact page in your PDF.

---

## How Ollama fits in

1. **Set up Ollama**:  
   - Installed with `curl -fsSL https://ollama.com/install.sh | sh`  
   - Pulled a local model: `ollama pull mistral` or `ollama pull llama3`  
   - Tested standalone:  
     ```bash
     ollama run mistral
     ```

2. **Integrated with the app**:  
   - The `ollama` Python client connects to `localhost:11434`.
   - When you ask a question, your local vector DB finds the top chunks.
   - The top chunks are used as **context** in the prompt.
   - Ollama answers *only* from that context â€” no internet calls.

---

## How Streamlit builds the UI

Your local app uses [Streamlit](https://streamlit.io/) to run like a mini web app:
- **PDF uploader**: drag & drop one or more PDFs.
- Shows **spinner/progress** while parsing, chunking & embedding.
- **Ask a question**: simple text box with a â€œthinkingâ€¦â€ spinner.
- **Prompt inspector**: shows what context the LLM sees.
- **Source Chunks**: see which text chunks answered your question.
- **Sidebar chat history**: so you can trace what you asked and what it answered.

---

## Project structure

```
chat-with-my-pdf/
â”œâ”€â”€ app.py                  # Your Streamlit web app
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parser.py           # Extracts text, tables, OCR
â”œâ”€â”€ pdfs/                   # PDFs you upload go here (local only)
â”œâ”€â”€ faiss.index             # Your local vector DB
â”œâ”€â”€ chunks_metadata.json    # Metadata to trace chunks to pages
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## How to run it

### 1ï¸âƒ£ Clone & set up your virtual environment

```bash
git clone https://github.com/Mrunmoy/chat-with-my-pdf.git
cd chat-with-my-pdf

python3 -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Run Ollama in the background

```bash
ollama serve
```
Or use the default `systemd` service if installed automatically.

---

### 3ï¸âƒ£ Start your Streamlit app

```bash
streamlit run app.py
```

It will open [http://localhost:8501](http://localhost:8501) â€” your local chat window.

---

### 4ï¸âƒ£ Upload, chat, repeat!

- Drag in one or more PDFs.
- Watch the spinner as they get parsed & chunked.
- A new vector DB is built every time.
- Ask any question â€” your local LLM answers using your PDFs only.

---

## Example: Local pipeline in action

![Multi PDF Upload Demo](docs/screenshot.png)

---

## Why this is private

- Your PDFs, index, and chunks stay local.
- Your `.gitignore` ensures you never commit private `pdfs/`, `faiss.index`, or `chunks_metadata.json`.
- Ollama runs your LLM offline.

---

## Built with

- [PyMuPDF](https://pymupdf.readthedocs.io/) â€” PDF parsing
- [pdfplumber](https://github.com/jsvine/pdfplumber) â€” Tables
- [pytesseract](https://github.com/madmaze/pytesseract) â€” OCR
- [sentence-transformers](https://www.sbert.net/) â€” Local embeddings
- [FAISS](https://github.com/facebookresearch/faiss) â€” Local vector DB
- [Ollama](https://ollama.com) â€” Run local LLMs
- [Streamlit](https://streamlit.io/) â€” Web UI

---

## âš–ï¸ License
MIT â€” for personal learning and research.  
Respect any copyright for your own PDFs.

---

Thatâ€™s it! ğŸ“šâœ¨  
**Upload â†’ Process â†’ Chat â†’ Learn â€” all local, all yours.**

